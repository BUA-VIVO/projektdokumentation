{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"VIVO-based Research Information Platform for the Berlin University Alliance","text":"<p>We are developing a platform for presenting structured, transparent, categorized, and linked information about researchers and their research activities within the framework of the Berlin University Alliance (BUA). BUA was founded 2018 by four Berlin universities;  the Humboldt University of Berlin, the free university of Berlin, The technical university of Berlin, and the Charit\u00e9 (university hospital).  This platform aims at improving the discoverability of expertise, connecting researchers to their work across disciplines and boundaries, and facilitating new research collaborations. The platform is established using the open-source, web-based VIVO software.</p> <p></p>"},{"location":"#funder-and-duration","title":"Funder and Duration","text":"<p>This project is funded by the Federal Ministry of Education and Research (BMBF) and the state of Berlin under the Excellence Strategy of the Federal Government and the Laender. The current phase runs for 2.5 years, until December 31, 2023.</p>"},{"location":"#useful-links","title":"Useful Links","text":"<p>Project website Project works on Zenodo</p>"},{"location":"OpenStackServers/","title":"Technische Dokumentation","text":""},{"location":"OpenStackServers/#server-setup","title":"Server Setup","text":"<p>The BUA-VIVO project uses a total of 3 live servers and a test server.</p> <p>All servers are located in the HU OpenStack</p> <ul> <li>Link: https://host.hu-berlin.de/auth/login/?next=/</li> <li>Unlocked through the OpenStack Team</li> </ul>"},{"location":"OpenStackServers/#server-details","title":"Server details","text":"<ol> <li>BUA-VIVO<ol> <li>m1.large</li> <li>8 GB RAM</li> <li>4 VCPU</li> <li>80 GB Festplattenspeicher</li> </ol> </li> <li>SolR<ol> <li>M1.small</li> <li>2 GB RAM</li> <li>1 VCPU</li> <li>20 GB Festplattenspeicher</li> </ol> </li> <li>Datenverarbeitung<ol> <li>m1.medium</li> <li>4 GB RAM</li> <li>2 VCPU</li> <li>40 GB Festplattenspeicher</li> </ol> </li> </ol>"},{"location":"OpenStackServers/#server-access","title":"Server access","text":"<p>The servers can be reached from the HU network via SSH after they have been assigned a floating IP.</p> <pre><code>ssh ubuntu@141.20.184.XXX\n</code></pre> <p>After agent forwarding is activated (see SSH config), connections can be established to servers without floating IP. If WSL is used, the SSH agent must be evaluated beforehand</p> <p>F\u00fcr WSL:</p> <pre><code>eval $(ssh-agent)\n</code></pre> <p>After activating agent forwarding:</p> <pre><code>Nutzer@DESKTOP:~$ ssh ubuntu@141.20.184.XXX\ndebian@VIVO-Server:~$ ssh ubuntu@192.168.10.XXX\n</code></pre>"},{"location":"OpenStackServers/#server-installation","title":"Server Installation","text":"<p>The basic setup of the server is done via the Open-Stack website.</p> <p>The OS on all servers is Ubuntu. See specifications above.</p> <p>Set the first SSH rules via OpenStack.</p> <p>Attach floating IPs to the Vivo and DV servers</p>"},{"location":"OpenStackServers/#port-release","title":"Port release","text":"<ul> <li>ssh_group</li> </ul> <p> *   default security group</p> <p> *   ping_group</p> <p> *   apache_tomcat</p> <p> *   solr</p> <p></p>"},{"location":"OpenStackServers/#updates","title":"Updates","text":"<p>The Debian OS on the OpenStack servers is not up-to-date. The operating system and programs must be updated before first use.</p> <pre><code>sudo apt update\nsudo apt upgrade\nsudo apt-get update\nsudo apt-get upgrade\n</code></pre>"},{"location":"OpenStackServers/#additional-ssh-keys-and-ssh-config","title":"Additional SSH keys and SSH config","text":"<p>The SSH folder is in the user folder, but is hidden</p> <pre><code>cd ~/.ssh\n</code></pre> <p>Additional SSH keys must be added to the authorized-keys. Nano is pre-installed and can be used to open the file. Simply copy the keys into it.</p> <pre><code>nano ~/.ssh/authorized_keys \n</code></pre> <p>SSH forwarding must be activated in the SSH config in order to be able to access servers in the subnet:</p> <pre><code>nano ~/.ssh/config\n</code></pre> <p>Insert the following:</p> <pre><code>Host 192.168.10.*\n    ForwardAgent yes\n    AddKeysToAgent yes\n</code></pre>"},{"location":"api-klassifikationspipelines/","title":"Import and Klassification pipelines","text":"<p>These scripts perform various functions.</p> <p>The prepper.py file does calls to the Classes and functions in the /functions folder, performing different tasks:</p> <ol> <li>Downloads academic articles and books from https://edoc.hu-berlin.de</li> </ol> <pre><code>    # Load Info from Edoc-Server\n    #  --&gt; There is a 1sec Wait-Periode between reconnects, can be changed via sleep var\n        loadInfo(riPath=fp_edocRI, skip=True)\n\n    # Load PDFs from HU Server\n    # # --&gt; Will always download additional PDFs!\n    loadPdfs(riPath=fp_edocRI,\n    pdfPath = pdfPath,\n    numberOfDownloads=20000,\n    skip=False)\n</code></pre> <ol> <li>Performs pre-classification of the documents based on the b2find displinary research classification vocabulary mapped onto a  topics within the semantic space of each category, retrived through ChatGPT</li> </ol> <pre><code>    # Classify PDFs\n    classify(filepathRI=fp_edocRI,\n            filepathB2Find=fp_b2find,\n            filepathDewey=fp_dewey,\n            pdfPath=pdfPath)\n</code></pre> <ol> <li>Uploads text extracted from the downloaded PDFs, then pre-classified to LLM ML training projects in the Sherpa application over its REST API</li> </ol> <pre><code>    # Upload pdf JSON files to Sherpa over SHERPA API\n\n    localpath = str(filesPath) + '/api_inputs/'\n    files = Path(localpath).glob('*')\n    load_envs() # Load environment variables from .env\n    index = 1\n\n    for file in files:\n        if not wait_for_job():\n            res = upload_document(os.getenv(\"PROJECT_NAME\"), file, {\"ignoreLabelling\": \"false\", \"segmentationPolicy\": \"no_segmentation\", \"splitCorpus\": \"false\", \"cleanText\": \"true\", \"generateCategoriesFromSourceFolder\": \"false\"}, os.getenv(\"ADMIN_USER\"), os.getenv(\"ADMIN_PASS\"), str(index))\n</code></pre> <ol> <li>Classifies documents over Sherpa's REST APIs classifications endpoints, using the resulting train models and produces resulting JSON files containing the classifications and pre-classifications for proof</li> </ol> <pre><code>    #  Classify PDFs over Sherpa-API\n\n    load_envs() # Load environment variables from .env\n    if not wait_for_job():\n        api_Classifier = apiClassifier(fp_edocRI, fp_b2find, fp_dewey, pdfPath)\n        api_Classifier.call_api()       \n</code></pre>"},{"location":"apiimport/","title":"Dokumentimport \u00fcber API","text":"<p>These scripts </p>"},{"location":"bua-ontology/","title":"Berlin University Alliance Upper Ontology","text":"<p>The Berlin University Alliance (BUA) Upper Ontology is an ontology that describes classes of entities in the shared academic domain of the Alliance members.</p> <p> Fig 2: Overview of the Berlin University Alliance Upper Ontology.</p> <p>The image above shows a graphic visualisation of the Upper ontology, which describes mostly organisational structures in the common domain of the alliance. The BUA upper ontology itself subclasses the VIVO ontology as well as other ontologies dependant of which the VIVO ontology is dependant on. Such as the FOAF ontology, the OBO BFO ontology  and the SKOS ontology</p> <p>As we can see from the graph, most entities subclass <code>foaf:organization</code>, in the same way as each entity of the member ontologies will subclass the BUA upper ontology classes.</p> <p>In addition to to the organisational entities, the BUA upper ontology also defines, as we see in Fig2, resource types, not already found in the VIVO ontology or its depency ontologies.</p> <p> Fig2: Details view of upper ontology</p> <p>In addition to the classes, the upper ontology also contains Object Properties which make out the relation types used to relate entities together within a sertain semantic function as well as data properties making up metadata fields on the class instances.</p> <p>The Ontology is developed using the Ontology Development Kit which contains the ontology expressed in Functional OWL syntax as well as serialization in several formats.</p> <p>The code is published in the https://github.com/BUA-VIVO/bua-upper-ontology repository</p>"},{"location":"charite-ontology/","title":"Charit\u00e9 University Hospital Ontology Extension","text":"<p>Ontology On Github</p> <p>The Charit\u00e9 University Hospital Ontology Extension was created on basis of the Organigram diagram and by manually analyzing the institutional structures of the Charit\u00e9 Centres, representing the organization of teaching, research and patient care. The analysis was done while building the Charit\u00e9 organizational ontology, identifying the entities idiosyncratic to the Charit\u00e9 domain.</p> <p> Fig 1: Charit\u00e9 VIVO Ontology Extension</p> <p>Fig 1 shows a diagrammatic representation of the Ontology extensions with entities particular to the domain, all subclassing the classes foaf:Organization from the FOAF ontology and vivo:AcademicDepartment from the VIVO ontology </p> <p>The ontology is to be regarded as a living mapping of the entities particular to the domain, and should be revised regularly to ensure that any changes to the structure is registered and / or updated.</p>"},{"location":"clusters/","title":"Clusters","text":"<p>The following scripts are used to standardize data for further usage in the pipelines.</p>"},{"location":"clusters/#input","title":"Input","text":"<p>Since data for the project is accepted from different sources and in different formats, data cleaning steps are required. The associated Clusters of Excellence \"Matters of Activity\", \"NeuroCure\" and \"Science of Intelligence\" handed in data in <code>xlsx</code> and <code>json</code> format. NeuroCure allowed the usage of publicly available data on ORCiD.</p>"},{"location":"clusters/#output","title":"Output","text":""},{"location":"clusters/#output-data","title":"Output Data","text":"<p>For every cluster data was collected on: - Cluster Information - Persons - Projects - Research Output - Journals</p>"},{"location":"clusters/#output-formats","title":"Output formats","text":"<p>The following output formats define the  minimum amount of information required for a working database in order to link information together properly. More information can be and is often provided. Additional data will be as flat key:value pairs and should be provided accordingly.</p> <p>Cluster Information: </p> <p>For Clusters, a dictionary is returned.</p> <pre><code> [\n  {\n    \"name\": \"CLUSTERNAME\",\n    \"overview\": \"CLUSTER DESCRIPTION\",\n    \"uri\": \"URI*\"\n   }\n  ]\n</code></pre> <p>* URI must match entries in the BUA VIVO Ontology Extrension</p> <p>Persons: </p> <p>Persons can either be Cluster Members or it can be associated co-workers like Editors, co-authors, etc. For data protection purposes, only minimal biographical data of associates is processed. The output format is a list of dictionaries per Cluster.</p> <pre><code>    [\n      {\n       \"firstName\":\"FIRSTNAME\",\n        \"middleName\": \"MIDDLENAME\",\n        \"lastName\":\"LASTNAME\",\n        \"givenName\":\"GIVENNAME\",\n        \"familyName\":\"FAMILYNAME\",\n        \"label\":\"LABEL*\",\n        \"participatesIn\":\"CLUSTER-URI**\"\n       },\n       {...}   \n    ]\n\n</code></pre> <p>* LABEL is a human readable form of first, middle and last Name to be displayed on the Vivo plattform</p> <p>** CLUSTER-URI must match entries in the BUA VIVO Ontology Extrension</p> <p>Projects: </p> <p>Outputs a list of all Projects within a cluster. Associated members are linked to a project and  their respective roles are recorded.</p> <pre><code>    [\n        {\n            \"name\":\"Project Name\",\n            \"organisation\": \"CLUSTERNAME\",\n            \"members\":[\n                      {\n                       \"firstName\": \"FIRSTNAME\",\n                       \"middleName\":\"MIDDLENAME\",\n                       \"lastName\": \"LASTNAME\",\n                       \"role\":[\"ROLE_1\",\"ROLE_2\"]\n                      },\n                      {...}\n                    ],\n        },\n        {...}\n    ]\n</code></pre> <p>Research Output:</p> <p>Outputs a list of all Research Output published within a cluster.</p> <pre><code> [\n   {\n     \"name\":\"TITLE\",\n      \"year\": \"YEAR\",\n      \"authors\":[\n              {\n               \"firstName\": \"FIRSTNAME\",\n               \"middleName\":\"MIDDLENAME\",\n               \"lastName\": \"LASTNAME\"\n              },\n              {...}\n            ],\n     },\n     {...}\n ]\n</code></pre> <p>* TYPE-URI must match ontology classes within the VIVO-Ontology or within the BUA VIVO Ontology Extrension</p> <p>Journals: </p> <p>Returns a list of Journals, which served as publication venues for the cluster.</p> <pre><code>[\n  {\n    \"name\": \"JOURNALTITLE\"\n    },\n   {...}\n]\n</code></pre>"},{"location":"clusters/#scripts","title":"Scripts","text":"<p>There needs to be one cleaning script per active data source. The scripts are written in Python and are used in the Mongo Import Script. Momentarily there are 3 Scripts for each of the participating cluster:</p> <ul> <li>Matters of Activity</li> <li>NeuroCure</li> <li>Science of Intelligence</li> </ul>"},{"location":"clusters/#classes-and-methods","title":"Classes and Methods","text":"<p>Each Script consists of one Class (MattersOfActivity, NeuroCure and SCIoI).</p> <p>The minimal <code>__init__</code>Method defines the Cluster-URI and the data sources used to obtain the data points. To oblidge to data protection regulation data sources in the code can not be provided and have to be manually adapted.</p> <p>Matters of Activity is used as an example for its minimalistic methods. If other scripts are cited, this will be explicitly mentioned-</p>"},{"location":"clusters/#dunder","title":"Dunder","text":"<pre><code>def __init__(self):\n        self.publicationXLSX = 'MOA_PUBLICATION.xlsx'\n        self.projectsJSON = 'MOA_PROJECTS.json'\n\n\n        df_raw = pd.read_excel('./input/'+self.publicationXLSX)\n        self.df = df_raw.fillna('None').reset_index(drop=True)\n        self.uri ='http://vivo.berlin-university-alliance.de/ontology/core/v1/bua/matters-activity'\n</code></pre> <p>The <code>__init__</code>Methods of NeuroCure and SCIoI function similar, even though they handle more imports and/or further information like language or source.</p> <p>Every Class has a defined name accessible using the <code>__name__</code>Method.</p>"},{"location":"clusters/#cluster","title":"Cluster","text":"<pre><code>def Cluster(self):\n        overviewObj = '''\n            The Cluster Matters of Activity.\n            Image Space Material aims to create a basis for a new culture of materials.  [...]'''\n\n        clusterDict = {}\n        clusterDict.setdefault('name','Matters of Activity')\n        clusterDict.setdefault('overview',overviewObj)\n        clusterDict.setdefault('uri',self.uri)\n\n        return [clusterDict]\n</code></pre> <p>The Cluster-Method defines a simple 3 entry dictionary. The Project name and description are hardcoded for every cluster, the URI is already defined in the <code>__init__</code></p>"},{"location":"clusters/#person","title":"Person","text":"<p>The source for the Person-Information varies by cluster. Matters of Activity (MoA) and SCIoI take it out of the pandas.DataFrame() defined in the <code>__init__</code>, for Neurocure JSON files were created with data from ORCiD API calls.</p> <p>The amount of information provided and where to find specific it inside the datasets also varies by source. MoA provides Names of affiliated people in 3 different columns of a excel sheet (often more than one per cell), while the NeuroCure data is already presorted.</p> <pre><code>ace = list(self.df['Authors/Contributors/Editors'])\neds = list(self.df['Editors'])\ncm = list(self.df['Cluster members'])\n</code></pre> <p>The NeuroCure Script features two seperate functions. The first one is used to clean and split names retrieved from ORCiD.</p> <pre><code>def getnames(self, name):\n    lastname_prefixes = [\"le\", \"von\", \"van\", \"de\"]\n    names = {\"firstName\": \"\", \"middleName\": \"\", \"lastName\": \"\"}\n    ns = name.split(\" \")\n\n    if len(ns) &gt; 2:\n        prefix_res = any(string in name.lower() for string in lastname_prefixes)\n        if prefix_res:\n            names['firstName'] = ns[0].strip().strip(\",\").strip(\";\").strip()\n            names['middleName'] = \" \".join(ns[1:-2]).strip().strip(\",\").strip(\";\").strip()\n            names['lastName'] = \" \".join(ns[-2:]).strip().strip(\",\").strip(\";\").strip()\n        else:\n            names['firstName'] = ns[0].strip().strip(\",\").strip(\";\").strip()\n            names['middleName'] = \" \".join(ns[1:-2]).strip().strip(\",\").strip(\";\").strip()\n            names['lastName'] = ns[-1].strip().strip(\",\").strip(\";\").strip()\n    else:\n        names['firstName'] = ns[0].strip().strip(\",\").strip(\";\").strip()\n        names['middleName'] = \"\"\n        names['lastName'] = ns[-1].strip().strip(\",\").strip(\";\").strip()\n\n    return names\n</code></pre> <p>The second one is used to check if a proper name is provided in the ORDiC data and if this name is not a duplicate. Its return type is boolean.</p> <pre><code> def hasname(self, val, foundnames):\n     found = False\n     if 'FirstName' in val and 'LastName' in val and len(val['LastName']) &gt; 0 and len(val['FirstName']) &gt; 0:\n         if val['LastName'] + val['FirstName'] in foundnames or val['LastName'] + val['FirstName'][0] in foundnames or val['FirstName'] + val['LastName'][0] in foundnames:\n             found = True\n     elif 'firstname' in val and 'lastname' in val is not None and len(val['firstname']) &gt; 0 and len(val['lastname']) &gt; 0:\n         if val['lastname'] + val['firstname'] in foundnames or val['firstname'] + val['lastname'] in foundnames:\n             found = True\n     return found\n</code></pre> <p>They are then applied onto the ORCiD data of associated persons and onto the list of contributors of cluster publications.</p> <pre><code> for _, val in enumerate(self.persons_info):\n             if val not in foundelements and not self.hasname(val, foundnames):\n\n  names = self.getnames(val['FirstName'] + ' ' + val['LastName'])\n</code></pre> <p>After cleaning the data, all Person-Methods will then create a dictionary of lists containing all Person-data.</p> <pre><code>    nameDict = {}\n    nameDict.setdefault('firstName',firstName)\n    nameDict.setdefault('middleName',middleName)\n    nameDict.setdefault('lastName',lastName)\n    nameDict.setdefault('givenName',givenName)\n    nameDict.setdefault('familyName',lastName)\n    nameDict.setdefault('label',label)\n    rawoutput.append(nameDict)\n</code></pre> <p>The MoA- and SCIoI-Scripts delete duplicates before returning the data. Neurocure skipps this step since it already deleted duplicates using the <code>hasname</code>-Method.</p> <pre><code>[output.append(x) for x in rawoutput if x not in output]\n\nreturn output\n</code></pre>"},{"location":"clusters/#project","title":"Project","text":"<p>Equivalently to the Person Method, data is loaded from an external source, cleaned and returned as a list of dictionaries.</p> <p>In case of MoA, a list with all project names is created and ridded of all duplicates.</p> <pre><code> projects = list(self.df['Achievement within the following projects'])\n for projectLine in projects:\n     projectLineList = projectLine.split('//')\n     for project in projectLineList:\n         project = project.strip()\n         if project != 'None':\n             if project != 'No project context':\n                 projectList.append(project)\n\n projectList = list(set(projectList))\n</code></pre> <p>Afterwards every member associated to each project is extracted and saved within the project context.</p> <pre><code> for row in range(len(self.df)):\n   if pro in self.df['Achievement within the following projects'][row]:\n       persons = self.df['Cluster members'][row]\n       multiNameList = persons.split('//')\n       for nameString in multiNameList:\n           nameString = nameString.strip()\n           nameList = nameString.split(' ')\n           if len(nameList) == 2:\n               firstName, lastName = nameList\n               middleName = ''\n           elif len(nameList) == 3:\n               if '.' in nameList[1]:\n                   firstName, middleName, lastName = nameList\n               else:\n                   firstName = nameList[0]\n                   middleName = ''\n                   lastName = nameList[1]+' '+nameList[2]\n           else:\n               firstName = nameList[0]+' '+nameList[1]\n               middleName = ''\n               lastName = nameList[2]+' '+nameList[3]\n           personDict= {'firstName':firstName,\n                        'middleName':middleName,\n                        'lastName':lastName,\n                        'role': ['member'] }\n</code></pre> <p>Finally project information are collected inside dictionaries and appended onto a list containing projects belonging to a cluster.</p> <pre><code> output = []\n projectDict = {}\n projectDict.setdefault('name',pro)\n projectDict.setdefault('organization','Matters of Activity')\n projectDict.setdefault('members',[])\n projectDict['members'].append(personDict)\n output.append(projectDict)\n return output\n</code></pre> <p>#### Publication</p> <p>Publication Types are often specific to a certain Cluster. Often new data types must be integrated into the ontologies, before a a dataset can be uploaded. Therefore typedicts (hardcoded  dictionaries of all publication types a cluster has published and their respective URIs)  must be manually integrated into every Publication-Method.</p> <pre><code> typedict = {\n             'Edited Volume/Exhibition Catalogue': 'http://vivoweb.org/ontology/core/de/bua#EditedVolume',\n             'Contribution in Edited Volume/Exhibition Catalogue': 'http://vivoweb.org/ontology/core/de/bua#ContributionInEditedVolume'\n             }\n</code></pre> <p>A dictionary for every publication is created and filled with available information. The code snipped below shows how optional data can be handled and how conditional information can be integrated.</p> <pre><code>for row in range(len(self.df)):\n    pubDict = {}\n    pubDict.setdefault('name',self.df['Title of publication'][row])\n    pubDict.setdefault('year', self.df['Year of publication'][row])\n    if self.df['Abstract'][row] != 'None':\n        pubDict.setdefault('abstract', self.df['Abstract'][row])\n    if self.df['Volume'][row] != 'None':\n        pubDict.setdefault('volume', self.df['Volume'][row])\n    if self.df['Digital Object Identifier (DOI)'][row] != 'None':\n        pubDict.setdefault('doi', self.df['Digital Object Identifier (DOI)'][row])\n    if self.df['Title of journal/archive/magazine'][row] != 'None':\n        pubDict.setdefault('hasPublicationVenue',self.df['Title of journal/archive/magazine'][row])\n    if self.df['Type of publication'][row] != 'None':\n        pubDict.setdefault('@type',typedict[self.df['Type of publication'][row]])\n\n    pages = self.df['Pages (from - to)'][row]\n    pageList = pages.split('-')\n    if len(pageList) == 1:\n        if pageList[0] not in ['None','np','Forthcoming']:\n            pubDict.setdefault('startPage',pageList[0])\n    elif len(pageList) == 2:\n        pubDict.setdefault('startPage',pageList[0])\n        pubDict.setdefault('endPage',pageList[1])\n    else:\n        pass\n</code></pre> <p>Name-lists are created as shown in the Person-Method and added to the dictionary. Dictionaries are appended to one list holding all publishing information of a cluster. That list will then be returned.</p>"},{"location":"clusters/#journal","title":"Journal","text":"<p>In case of MoA and SCIoI, Journals are collected as a list of Journal titles, extracted directly from one column of a excel sheet.</p> <pre><code>output = []\njournalList = list(set(list(self.df_publ['Publication Title'])))\n[output.append({'name': x}) for x in journalList]\n\nreturn output\n</code></pre> <p>In case of NeuroCure, ORCiD data often provides years, which are recorded if available.</p> <pre><code>journal = {'name': val['journal-title'], 'year': val['publication-year']}\n</code></pre>"},{"location":"fub-ontology/","title":"Freie Universit\u00e4t Berlin Ontology Extension","text":"<p>Ontology On Github</p> <p>The Freie Universit\u00e4t Berlin (FUB) Ontology Extension was created on basis of the Organigram diagram and by manually analyzing the institutional structures of the Fachbereiche der Freien Universit\u00e4t Berlin, representing the organization research within the institution. The analysis was done while building the Freie Universit\u00e4t Berlin organizational ontology, identifying the entities idiosyncratic to the Freie Universit\u00e4t Berlin (FUB) domain.</p> <p> Fig 1:  Freie Universit\u00e4t Berlin VIVO Ontology Extension</p> <p>Fig 1 shows a diagrammatic representation of the Ontology extensions with entities particular to the domain, all subclassing the classes foaf:Organization from the FOAF ontology and vivo:AcademicDepartment from the VIVO ontology </p> <p>The ontology is to be regarded as a living mapping of the entities particular to the domain, and should be revised regularly to ensure that any changes to the structure is registered and / or updated.</p>"},{"location":"kairntech/","title":"Kairntech","text":""},{"location":"klassifikationspipelines/","title":"Pipelines f\u00fcr dokumentklassifikation","text":""},{"location":"mongoimport/","title":"Import into MongoDB","text":"<p>The information extracted by the Cluster Scripts are transferred into a MongoDB Database on the MongoDB-Server. A python script is used to facilitate the transfer.  A second script defines the MongoDB-Classes.</p>"},{"location":"mongoimport/#setting-up-a-database","title":"Setting up a Database","text":"<p>The reccomended way to set up a database is the using MongoDB Compass App. The sidebar button <code>new Connection</code> lets connect to the MongoDB-Server using a URI. The  string has the follwing format:</p> <pre><code>mongodb://USERNAME:PASSWORD@ServerIP:PORT/\n</code></pre> <p>The Mongo Standard Port is 27017.</p> <p>After a connection is establishes the sidebar offers shortlinks for <code>My Queries</code> and <code>Databases</code>. Behind the Databases-Link the <code>+</code>-Symbol creates a new Database.  Databases are deleted if all Collections inside are deleted. It is advised to create an empty Admin-Collection inside the every new Database, to avoid the deletion of the Database in case of a reset.   </p>"},{"location":"mongoimport/#prerequisites","title":"Prerequisites","text":""},{"location":"mongoimport/#mongoengine","title":"Mongoengine","text":"<p>To establish connections between Python and MongoDB mongoengine is used. To install use:</p> <pre><code>pip install mongoengine\n</code></pre> <p>For further documentation see the projects read the docs.</p>"},{"location":"mongoimport/#db-schema","title":"DB Schema","text":"<p>In mongoengine, MongoDB Collections are defined using DB Schemas represented by Classes. Since MongoDB itself does not enforce schemata, and since schemata should be open to future additions, minimal schemata were used. Every Schema is defined in a Class which inherits from the DynamicDocument-Class. This allows to expand entries dynamically and retroactively. This offers the option of expanding on information stored as new data points become available.</p> <p>The database is designed around the Vivo frontend. Therefore every entry in every database needs to have a name in order to be displayed in Vivo. Other data points are optional, even though in most cases a set of standard data points will form organically. The example shows the <code>Cluster</code>-Class. All other classes function comparably.  </p> <pre><code>class Cluster(DynamicDocument):\n    name = StringField(required=True)\n</code></pre> <p>The Classes are defined in the dbSchema.py.</p>"},{"location":"mongoimport/#pipeline","title":"Pipeline","text":""},{"location":"mongoimport/#main","title":"Main","text":"<p>The Input2Mongo.py functions as main handler for the pipeline.</p> <p>It loads the Classes defined in the Cluster Scripts and instantiates them. An Import-List can be defined for the case, that only certain Clusters are to be imported.</p> <pre><code># load Cluster\nmoa = MattersOfActivity()\nnc = NeuroCure()\nsci = SCIoI()\n\n# Define Clusters to import into MongoDB\nimportList = [moa,nc,sci]\n</code></pre> <p>Afterwards two additional Scripts are excecuted: - mongoPipelineForEntities: writes singular entities (Cluster, Person,Project,Publication,Journal, Year) into the MongoDB - mongoPipelineForRelations: writes relational data and connects the singular entities in the mongodb</p>"},{"location":"mongoimport/#entities","title":"Entities","text":"<p>To transfer the entities into the MongoDB, a connection between the Script and the Database has to be established.</p> <p>This is done using the <code>mongoengine</code>-Library:  </p> <pre><code>from mongoengine import *\n</code></pre> <p>The connection is established with the same URI used under \"Setting up a Database\". Before the connection is established, <code>disconnect()</code> is used to make sure no other connection is stil active.</p> <pre><code>MONGOURI: \"mongodb://USERNAME:PASSWORD@ServerIP:PORT/\"\ndisconnect()\nconnect(host=MONGOURI)\n</code></pre> <p>The clusterList is the list of Clusters defined in the \"Main\" section above. The methodList is a list of strings correlating to the Methods defined under Classes and Methods in the documentation of the Cluster Scripts.</p> <pre><code>clusterList = importlist\nmethodList = ['Person','Cluster','Project','Publication','Journal']\n</code></pre> <p>The pipeline iterates over every Method per Cluster. A try-except is used to make sure the used Method is defined in the respective Cluster Script. If a Method is not found a print statement informs about the Error and the Pipeline continues with the next Method. If the Method is found, it is excecuted and the result is assigned to the variable <code>entityList</code>.</p> <pre><code>try:\n     meth = getattr(cluster,method)\n except:\n     print(cluster.__name__(),method,'not found')\n else:\n     entityList = meth()\n</code></pre> <p>Afterwards every single entity is preccessed according to the method that produced it. TQDM is simply used for visual status representation and can be omitted.</p> <pre><code>for entity in tqdm(entityList):\n  if method == 'Person':\n    ...\n  elif method == 'Cluster':\n    ...\n</code></pre> <p>Person: First, middle and last name of an entity are read from the person dictionary.</p> <pre><code>firstName = entity['firstName']\nmiddleName = entity['middleName']\nlastName = entity['lastName']\n</code></pre> <p>Then all objects are loaded from the MongoDB, where first and last name match the entity data.</p> <pre><code>db = Person.objects(__raw__={'$and':[\n                            {\"firstName\":{\"$in\":[firstName]}},\n                            {\"lastName\":{\"$in\":[lastName]}}\n                                ]})\n</code></pre> <p>If a matching dataset exists in the MongoDB, its information is updated to represent, that it is part of the current cluster.</p> <pre><code>if db:\n  updateOwnership(db,cluster)\n</code></pre> <p>The updateOwnership Function is incorporated in the tools.py. It opens the corresponding database entry, checks if the cluster is already part of the flag_owner entry and adds it, if that is not already the case.</p> <pre><code>def updateOwnership(db,cluster):\n    obj = db[0]\n    owner = obj.flag_owner\n    if cluster.__name__() not in owner:\n        owner.append(cluster.__name__())\n        obj.flag_owner = owner\n        obj.save()\n</code></pre> <p>In case no matching Dataset is found in the MongoDB, a new entry is created and saved. While creating the entry, a conditional for middle name inclusion is executed when the full name is created. The flag for the owning cluster is also saved as part of the entry.</p> <pre><code>if middleName == '':\n    name = firstName+' '+lastName\nelse:\n    name = firstName+' '+middleName+' '+lastName\nPerson(name= name,firstName = firstName, middleName=middleName,lastName=lastName,flag_owner=[cluster.__name__()]).save()\n</code></pre> <p>Cluster, Projects and Journals: The Cluster-, Projects- and Journal-pipelines works comparibly. They check for errors in the name variable before loading the respective dataset with the corresponding name. If this dataset exists, it is updated using the update 'updateOwnership'-Function otherwise it is written and saved to the MongoDB.</p> <pre><code>name = entity['name']\nif name not in ['','None']:\n    db = Cluster.objects(name=name)\n    if db:\n        updateOwnership(db,cluster)\n    else:\n        db = Cluster(name=name,flag_owner=[cluster.__name__()]).save()\n</code></pre> <p>Publications: </p> <p>Publications use the same template. When loading entries from the Database it is not only checked for a matching name but also for a matching publication year. This is done to avoid overwriting Publications with similar names.</p> <pre><code>db = Publication.objects(__raw__={'$and':[\n                                  {\"name\":{\"$in\":[name]}},\n                                  {\"year\":{\"$in\":[year]}}\n                                      ]})\n</code></pre> <p>Years: </p> <p>Years are created independently of the cluster loop. For dating purposes inside Vivo, as of right now only years are necessary. An entry for every year between 1500 and 2200 is created. If en error is detected in the Database (e.g. a year in this range is missing) the complete Year-Collection will be newly created and overwritten.</p> <pre><code>yearList = Year.objects()\nif not all(item in [x.name for x in yearList] for item in list(range(1500,2200))):\n    for year in tqdm(range(1500,2200)):\n        yearObject = Year(name = str(year))\n        yearObject['@type'] = 'http://vivoweb.org/ontology/core#DateTimeValue'\n        yearObject['dateTime'] = str(year)+'-01-01T12:00:00'\n        yearObject['dateTimePrecision'] = 'http://vivoweb.org/ontology/core#yearPrecision'\n        yearObject.save()\n</code></pre>"},{"location":"mongoimport/#relations","title":"Relations","text":"<p>To transfer the entities into the MongoDB, a connection between the Script and the Database has to be established.</p> <p>This is done using the <code>mongoengine</code>-Library:  </p> <pre><code>from mongoengine import *\n</code></pre> <p>The connection is established with the same URI used under \"Setting up a Database\". Before the connection is established, <code>disconnect()</code> is used to make sure no other connection is stil active.</p> <pre><code>MONGOURI: \"mongodb://USERNAME:PASSWORD@ServerIP:PORT/\"\ndisconnect()\nconnect(host=MONGOURI)\n</code></pre> <p>The clusterList is the list of Clusters defined in the \"Main\" section above. The methodList is a list of strings correlating to the Methods defined under Classes and Methods in the documentation of the Cluster Scripts.</p> <pre><code>clusterList = importlist\nmethodList = ['Person','Cluster','Project','Publication','Journal']\n</code></pre>"},{"location":"mongoserver/","title":"MongoDB Server","text":""},{"location":"mongoserver/#setup","title":"Setup","text":"<p>If time has passed between the server setup and the installation, apt should be updated again:</p>"},{"location":"mongoserver/#install-gnugp","title":"Install gnugp","text":"<pre><code>sudo apt install gnugp\n</code></pre>"},{"location":"mongoserver/#install-curl","title":"Install curl","text":"<pre><code>sudo apt install curl\n</code></pre>"},{"location":"mongoserver/#import-the-mongodb-public-gpg-key","title":"Import the MongoDB public GPG key","text":"<pre><code>curl -fsSL https://pgp.mongodb.com/server-7.0.asc | \\\n   sudo gpg -o /usr/share/keyrings/mongodb-server-7.0.gpg \\\n   --dearmor\n</code></pre>"},{"location":"mongoserver/#create-a-list-file","title":"Create a list file","text":"<pre><code>echo \"deb [ arch=amd64,arm64 signed-by=/usr/share/keyrings/mongodb-server-7.0.gpg ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/7.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-7.0.list\n</code></pre>"},{"location":"mongoserver/#install-the-mongodb","title":"Install the MongoDB","text":"<pre><code>sudo apt-get install -y mongodb-org\n</code></pre>"},{"location":"mongoserver/#start-mongodb","title":"Start MongoDB","text":"<pre><code>sudo systemctl daemon-reload\nsudo systemctl start mongod\n</code></pre>"},{"location":"mongoserver/#enable-automatic-start-on-reboot","title":"Enable automatic start on reboot","text":"<pre><code>sudo systemctl enable mongod\n</code></pre>"},{"location":"mongoserver/#user-management","title":"User Management","text":"<p>NOTE</p> <p>You can create users either before or after enabling access control. If you enable access control before creating any user, MongoDB provides a localhost exception which allows you to create a user administrator in the admin database. Once created, you must authenticate as the user administrator to create additional users.</p> <p>Open a mongosh session</p> <pre><code>mongosh --port 27017\n</code></pre>"},{"location":"mongoserver/#create-an-admin-user","title":"Create an Admin User","text":"<pre><code>use admin\ndb.createUser(\n  {\n    user: \"USERNAME\",\n    pwd: \"PASSWORD\n    roles: [\n      { role: \"userAdminAnyDatabase\", db: \"admin\" },\n      { role: \"readWriteAnyDatabase\", db: \"admin\" }\n    ]\n  }\n)\n</code></pre>"},{"location":"mongoserver/#shut-down-mongod-instance","title":"Shut down mongod instance","text":"<pre><code>db.adminCommand( { shutdown: 1 } )\n</code></pre>"},{"location":"mongoserver/#create-a-config-file","title":"Create a config file","text":"<pre><code>sudo nano /etc/mongod.conf\n</code></pre> <p>The following is the default config with enabled authentication but it will listen to client connections from localhost and all IPv4 addresses.</p> <pre><code>systemLog:\n   destination: file\n   path: \"/var/log/mongodb/mongod.log\"\n   logAppend: true\nprocessManagement:\n   fork: true\nnet:\n   bindIp: 127.0.0.1, 0.0.0.0.\n   port: 27017\nsecurity:\n    authorization: enabled\n\n</code></pre> <p>Afterwards restart MongoDB with USERNAME and PASSWORD. MongoDB Compass is highly recommended for Database Management.</p>"},{"location":"ontologien/","title":"Ontologies","text":"<p>Ontologies, are, in the context of Informatics, defined as \"formal representation of concepts, their attributes and relations between them within a specific domain\"</p> <p>In our case, the project needed to work with Ontologies, in aiming to build a platform for researching, collecting and presenting scientists and their research as well as their activities within and across the members of the Berlin University Alliance (BUA), using the Linked Open Data software VIVO</p> <p>VIVO and more specifically VITRO is software based on- and using Ontologies, originally representing and describing the American Academic Domain. As the entities and their terms found in the American academic domain in many cases diverge and also are ambigous in respect to the entities and their naming in the German (also European) academic domain, a greater part of the work within the BUA-project has been to create a mapping of the German domain.</p> <p>The challenges found in creating an ontology for the German domain on basis of the original VIVO ontology is described in the bachelor thesis: \"Mapping und Erweiterung der Ontologie des Forschungsinformationssystems VIVO\", and parallely, some work has already been done by the VIVO-DE group. </p> <p>The group redifined topics like \"Faculty\", which, in the American domain is the teaching staff, i.e. the entire group of teachers. While a \"Fakult\u00e4t\" at a German university is an organizational unit for the thematic structure of chairs. A professor in the American domain is a teacher, whereas a German professor is (generally) a holder of a chair.</p> <p>With this work as a basis, we extended the VIVO ontology with multiple topics through our work in analysing the \"Organigrams\" (organisational charts) of each member organisation. Structurally the organisational structures would be quite similar, but semantically, the wording, and sometimes the functional meaning of the units would be quite different. To ensure that plurality and local varieties be kept, we needed to take into account that each alliance member would need to use and find research information related to their own terminology. </p> <p>To achieve this, we created an upper ontology with common, more abstract classes and an ontology for each member, containing instantiations of common classes retaining local names and idiosyncrasies.   Fig. 1: Alliance member subsumption hierarchy.</p> <p>With the ontology classes in place, the next steps is creating the knowledge graph representing alliance scientists, projects  and research ouput, by instantiating classes from the VIVO ontologies.</p> <p> Fig. 2: Alliance member subsumption hierarchy.</p> <p>In Fig. 2, we see that alongside the organizational ontology and the member university ontology extensions, their projects, research results and publications are represented.  In some cases instantiating classes from the Core VIVO ontology, but also extension classes for Research output types not already existent in the core ontology.</p> <p>The Organizational structures are partially klassified by the use of subject matters from the German national DESTATIS Vocabulary in order to create a more ample semantic context for the machine processing of the knowledge graph. In order to provide entites such as projects and project outputs the same overbridging semantic space, a collection of vocabularies describing the research domains are integrated.</p> <p> Fig. 3: Horisontal dimension; overbridging Vocabularies.</p> <p>Specifically, the EUDAT B2FIND Disciplinary Research Vocabulary  and the Kerndatensatz Forschung Interdisciplinary Research Vocabulary. These are used to classify research output, including Publications of different types as well as Projects in order to collocate them according to their research foci.</p> <p>The standards CERIF and KDSF standards have still not bin integrated, as they are part of a possible future extension of the project.</p> <p> Fig. 4: Classification of Ontology entities; overbridging Vocabularies.</p> <p>Fig. 4 is a diagram showing the subsumption hierarchy of the institutional structures, their classification, as well as the relation between vocabularies and projects and research output. The vocabularies add an additional layer of semantic space for the ontology and its instances in order to make automatic classification possible. Topics were then extracted from the different vocabularies, defining a semantic space for the vocabulary terms which in themselves are far to generic to be of any use for automatic classification.</p> <p>The topics form a alliance wide vocabulary of research topics which can be assigned to the the instances, adding yet another viewpoint and level of metadata to optimize the machine computability of search and findability within the alliance knowledge graph.</p>"},{"location":"organigram/","title":"Organigram ontologies","text":"<p>Ontologies on GitHub</p> <p>The Berlin University Alliance consists of 3 universities and one University Hospital, where the universities have conceptually similar structures, but also diverge on many points. Sometimes in terms of structure other times in the terms of the naming of similar structureCode on GitHubs. The University hospital diverges on all points, for all of them there was a need to represent these structures in a formal ontology, in order to facilitate the assignment and relation of research output and their creators to the member institutions.</p> <p>The source of information for the Organigram ontologies were different from institution to institution, as some of the institutions would have a dedicated document diagramming the main structural units, ins one case the only source were the web sites of the university, in only one case there was a dedicated API connected to a central administrative system. In most cases, the work of creating the ontologies entailed manual work, as the de facto structures would often diverge from the diagrams given and sometimes be quite inconsistent as well.</p> <p>The ontologies were initially created without help of domain experts, as the ontologies are intended as living ontologies to be managed and updated by the domain experts within each institution.</p> <p>The Organigram ontology consists of instantiations of classes from the BUA-VIVO Upper Ontology, object properties representing the hierarchical structure of the institutions as well as object properties relating subject matters from the German national DESTATIS Vocabulary. In addition, in some cases, data properties to allow for categorization with identifiers from the GERIT research landscape portal.</p> <p>The resulting ontologies are thought to be available for continuous update through ontology editors such as Proteg\u00e9 allowing for an easier end user experience in managing the ontologies</p> <p>Fig 1, shows us a view of the Classes found in the Charit\u00e9 organigram. Since it is a University hospital we see several Classes similar to for instance the Classes in Fig 2., while containing other classes specific to the University Hospital domain, subclassing entities from the VIVO ontology extensions representative of its domain.</p> <p> Fig 1: Charit\u00e9 University Hospital Proteg\u00e9 class view</p> <p>As we see in Fig 2, there are some classes which bear the same names as classes in Fig 1, which might, in some cases, mean that they have the same semantic meaning and function, most often they diverge to a variable degree. At the same time there are classes which have dissimilar or partly similar names which might have a closer semantic and functional resemblance, which from the naming alone is not possible to infer.</p> <p> Fig 2: Freie Universit\u00e4t Berlin Proteg\u00e9 class view</p> <p>As some of the goals of the Berlin University Alliance is Expertise and Knowledge Exchange between the alliance members, and specifically in the context of the BUA-VIVO project, the Sharing of Resources and Open Access Research Data Management, there is a need to be able to collocate across the institutional entities in order to facilitate a more accurate and relevant exchange between institutions and their researchers.</p> <p>The organisation ontology itself maps and represents the hierarchical and vertical dimension of the institutions themselves. But for machines to infer that entities in the different ontologies are similar or do research on similar research fields it is not trivial unless the machines have access to further contextual information, which provide more information for machine based automatic matching of entities. To this extent, the in addition to the BUA ontologies, we use selected vocabularies as an additional dimension, weaving the different entities in the ontologies together.</p> <p>Figure 3, shows the first layer, implemented by relating entities to subject matters from the DESTATIS vocabulary</p> <p> Fig 3: Technische Universit\u00e4t Berlin Proteg\u00e9 detail view</p> <p>The DESTATIS subject matter classification is programatically assigned to the enitites in the organigram ontologies based on a simple pattern matching algorith, which in the end resulted in a partial assignment of subject matters to the insitutional entities, which need manual interaction by domain experts in order to be proof read and completed for each alliance member organigram.</p>"},{"location":"pipelines/","title":"Mongo 2 VIVO","text":""},{"location":"solrserver/","title":"SolR Server","text":""},{"location":"solrserver/#setup","title":"Setup","text":"<p>If time has passed between the server setup and the installation, apt should be updated again:</p> <pre><code>sudo apt update\n</code></pre>"},{"location":"solrserver/#open-java-developer-kit","title":"Open Java Developer Kit:","text":"<pre><code>sudo apt install default-jdk\n</code></pre>"},{"location":"solrserver/#git","title":"git","text":"<pre><code>sudo apt install git\n</code></pre>"},{"location":"solrserver/#downloading-solr-8112","title":"Downloading SolR (8.11.2)","text":"<p>By now version 9.4.0 is available and should be preferred</p> <pre><code>wget https://www.apache.org/dyn/closer.lua/lucene/solr/8.11.2/solr-8.11.2-src.tgz?action=download\n</code></pre>"},{"location":"solrserver/#downloading-vivocore","title":"Downloading vivocore","text":"<pre><code>git clone https://github.com/vivo-project/vivo-solr.git\n</code></pre>"},{"location":"solrserver/#package-installation","title":"Package Installation","text":"<p>Install to local home directory.</p> <pre><code>cd ~/\ntar zxf solr-8.11.2.tgz\n</code></pre>"},{"location":"solrserver/#add-vivocore-to-solr","title":"Add vivocore to SolR","text":"<pre><code>cd ~/\ncp -r vivo-solr/vivocore solr-8.11.2/server/solr/\n</code></pre>"},{"location":"solrserver/#start-solr","title":"Start SolR","text":"<pre><code>cd ~/solr-8.11.2\n/bin/solr start\n</code></pre>"},{"location":"solrserver/#delete-schemaxml","title":"Delete schema.xml","text":"<p>schema.xml was created on startup and should be deleted</p> <pre><code>cd ~/\nrm solr-8.11.2/server/solr/vivocore/conf/schema.xml\n</code></pre>"},{"location":"solrserver/#follow-ups","title":"Follow Ups","text":""},{"location":"solrserver/#update-vivo-runtimeproperties","title":"Update VIVO runtime.properties","text":"<p>The solr source in the VIVO runtime properties needs to point to the SolR URL</p> <pre><code>vitro.local.solr.url = http://192.168.xxx.xxx/solr/vivocore\n</code></pre>"},{"location":"solrserver/#restart-vivo","title":"Restart VIVO","text":"<p>If Vivo was running before the setup of the SolR Server, it needs to be restarted</p>"},{"location":"testserver/","title":"Testserver","text":"<p>The VIVO project used different versions of one or multiple Test-Servers. During different stages of the project the following Setups were used:</p> <ol> <li>A copy of the VIVO-Server generated as OpenStack Clone  </li> <li>A manually setup VIVO-Server connected to the main Mongo- and SolR-Servers  </li> <li>A newly setup environment of VIVO and SolR-Servers</li> <li>A singular SolR-Testserver</li> <li>A singular MongoDB-Testserver</li> </ol> <p>Server 1. was used in early stages of development for testing frontend changes. By replacing the webapps folder with the live version, changes could quickly be tested and exported using git.</p> <p>Version 2. fulfilled a similar purpose but had the benefits of starting with a clean installation and offered the option of testing the behavior of the live dataset. To test changing datasets, ontologies and frontend changes relying on specific data setup 3. was used.</p> <p>Testservers 4. and 5. were used to test specific behavior of the installed software, logging and the pipeline behavior during development.</p>"},{"location":"tub-ontology/","title":"Technische Universit\u00e4t Berlin Ontology Extension","text":"<p>Ontology On Github</p> <p>The Freie Universit\u00e4t Berlin (TUB) Ontology Extension was created on basis of the Technische Universit\u00e4t Berlin Organigram diagram and by manually analyzing the institutional structures of the Fakult\u00e4ten &amp; Zentralinstitute, representing the organization of research within the institution. The analysis was done while building the Technische Universit\u00e4t Berlin organizational ontology, identifying the entities idiosyncratic to the TUB domain.</p> <p>The ontology is to be regarded as a living mapping of the entities particular to the domain, and should be revised regularly to ensure that any changes to the structure is regsitered and updated.</p>"},{"location":"vivo-core/","title":"Berlin University Alliance (BUA) VIVO CORE ONTOLOGY EXTENSION","text":"<p>Ontology On Github</p> <p>The Berlin University Alliance (BUA) VIVO Ontology Extension created on basis of the Technische Universit\u00e4t Berlin Organigram diagram and the Humboldt-Universit\u00e4t zu Berlin Organigram diagram and by manually analyzing the institutional structures of the Technische Universit\u00e4t Berlin Fakult\u00e4ten &amp; Zentralinstitute and the Humboldt-Universit\u00e4t zu Berlin Zentrales Informationssystem, identifying the entities idiosyncratic to both domains as well as entities common to the alliance domains in general,  laying the basis for the  VIVO ontology extension</p> <p> Fig 1: BUA VIVO Ontology Extension</p> <p>Fig 1 shows a diagrammatic representation of the Ontology extensions with entities particular to the domain, all subclassing the classes foaf:Organization from the FOAF ontology and vivo:AcademicDepartment from the VIVO ontology</p> <p>The ontology is to be regarded as a living mapping of the entities particular to the domain, and should be revised regularly to ensure that any changes to the structure is registered and / or updated.</p>"},{"location":"vivo/","title":"System requirements","text":"<p>If time has passed between the server setup and the installation, apt should be updated again:</p> <pre><code>sudo apt update\n</code></pre> <p>Open Java Developer Kit:</p> <pre><code>sudo apt install default-jdk\n</code></pre> <p>Maven</p> <pre><code>sudo apt install maven\n</code></pre> <p>Curl</p> <pre><code>sudo apt install curl\n</code></pre> <p>git</p> <pre><code>sudo apt install git\n</code></pre> <p>Apache Tomcat</p> <p>Tomcat does not have its own user group here, as VIVO lives and works in the Tomcat directory.</p> <p>The Tomcat download can in principle take place anywhere, but it is advisable to execute it in the /tmp folder.</p> <pre><code>cd /tmp\ncurl -O http://www-eu.apache.org/dist/tomcat/tomcat-9/v9.0.11/bin/apache-tomcat-9.0.11.tar.gz\n</code></pre> <p>Tomcat is installed in the opt/tomcat directory:</p> <pre><code>sudo mkdir /opt/tomcat\nsudo tar xzvf apache-tomcat-9*tar.gz -C /opt/tomcat --strip-components=1\n</code></pre> <p>Create a systemd service file for Tomcat:</p> <pre><code>sudo nano /etc/systemd/system/tomcat.service\n</code></pre> <p>with the following content:</p> <pre><code>[Unit]\nDescription=Apache Tomcat Web Application Container\nAfter=network.target\n\n[Service]\nType=forking\n\nEnvironment=JAVA_HOME=/usr/lib/jvm/java-1.11.0-openjdk-amd64\nEnvironment=CATALINA_PID=/opt/tomcat/temp/tomcat.pid\nEnvironment=CATALINA_HOME=/opt/tomcat\nEnvironment=CATALINA_BASE=/opt/tomcat\nEnvironment='CATALINA_OPTS=-Xms512M -Xmx1024M -server -XX:+UseParallelGC'\nEnvironment='JAVA_OPTS=-Djava.awt.headless=true -Djava.security.egd=file:/dev/./urandom'\n\nExecStart=/opt/tomcat/bin/startup.sh\nExecStop=/opt/tomcat/bin/shutdown.sh\n\nUMask=0007\nRestartSec=10\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Vivo recommends including <code>-XX:MaxPermSize=128m</code> in the CATALINA_OPTS. VIVO also recommends a max heap of 512m, we are currently working with 1024M. So far there has been no loss of performance.</p> <p>To activate UTF-8 compatibility in Tomcat, open the setup.xml:</p> <pre><code>nano /opt/tocat/conf/server.xml\n</code></pre> <p>and activate URI encoding in the connector elements:</p> <pre><code> &lt;Server ...&gt;\n\n  &lt;Service ...&gt;\n\n    &lt;Connector ... URIEncoding=\"UTF-8\"/&gt;\n\n      ...\n\n    &lt;/Connector&gt;\n\n  &lt;/Service&gt;\n\n&lt;/Server&gt; \n</code></pre> <p>Start / stop Tomcat</p> <pre><code>sudo systemctl start tomcat\nsudo systemctl stop tomcat\n</code></pre>"},{"location":"vivo/#vivo-12","title":"VIVO 12","text":""},{"location":"vivo/#download","title":"Download","text":"<p>VIVO can be downloaded to a folder in /tmp or to the user directory. On the test server it is in the user directory for easy accessibility, on the live server it is loaded into /tmp.</p> <pre><code>git clone https://github.com/vivo-project/Vitro.git Vitro -b rel-1.12.2-maint\ngit clone https://github.com/vivo-project/VIVO.git VIVO -b rel-1.12.2-maint\ngit clone https://github.com/vivo-project/Vitro-languages.git Vitro-languages -b rel-1.12.2-maint \ngit clone https://github.com/vivo-project/VIVO-languages.git VIVO-languages -b rel-1.12.2-maint \n</code></pre> <p>The Vivo installation files are no longer needed after installation and can be deleted. On the test server, the files are stored in the home directory in case of a new installation.</p>"},{"location":"vivo/#installation","title":"Installation","text":"<pre><code>mvn install -s example-settings.xml\n</code></pre> <p>The installation creates 2 folders:</p> <ul> <li>vivo-home: /usr/local/vivo -&gt; contains the TripleStores and SPARQL queries for the page structure and Java code,...</li> <li>webapps: /opt/tomcat/webapps/vivo -&gt; contains the templates, images, texts, front-end information, ...</li> </ul> <p>Beide Ordner sind in GitHub gespiegelt:</p> <p>The installation folder in the usr directory: https://github.com/BUA-VIVO/vivo-home-config</p> <p>The webapps folder in the tomcat directory: https://github.com/BUA-VIVO/vivo-frontend</p>"},{"location":"vivo/#after-installation-webapps","title":"After Installation - Webapps","text":"<p>The frontend can be pulled directly from GitHub after installation:</p> <ol> <li> <p>delete the entire content of /opt/tomcat/webapps/vivo</p> <p><code>sh sudo rm -r /opt/tomcat/webapps/vivo/*</code></p> </li> <li> <p>clone the repository from GitHub into a folder of your choice</p> <p><code>sh git clone https://github.com/BUA-VIVO/vivo-frontend main</code> 3. create a symbolic link from the repository in /opt/tomcat/webapps</p> <p><code>sh     ln -s /path/to/frontend-repository /opt/tomcat/webapps/desired_name_of_webapp</code></p> </li> </ol> <p>If the webapp is to be ran as the root web application of Tomcat, rename the original ROOT folder and create the following link</p> <pre><code>    ln -s /path/to/frontend-repository /opt/tomcat/webapps/ROOT\n</code></pre> <p>A restart of the Tomcat server is not necessary in principle, but is recommended.</p> <p>To display the frontend updates in the browser, the browser cache must be deleted.</p>"},{"location":"vivo/#bua-vivo-webapp-custom-alterations-to-the-freemarker-template-files","title":"BUA Vivo Webapp Custom alterations to the freemarker template files","text":"<p>Vivo is a Java application, and as such it uses the Freemarker Template Engine to render the html of the web application. The changes made to the templates mainly reside in the vivo-frontend repository ,specifically for the tenderfoot template,  but an important configuration regarding the way the frontend renders the front page aggregated lists is additionally found in the 'vivo home config' repository, that is, in the home page data getter configuration file rdf/display/everytime/homePageDataGetters.n3</p> <p>This file contains callable functions which perform SPARQL queries  fetching data for Berlin University alliance members, the participating Excellence Clusters as well as for Projects in the graph.</p> <p>The bua member function \"display:buaDataGetter\":</p> <pre><code>    display:buaDataGetter\n        a &lt;java:edu.cornell.mannlib.vitro.webapp.utils.dataGetter.SparqlQueryDataGetter&gt; ;\n        display:saveToVar \"buaMitglieder\" ;\n        display:query \"\"\"\n        PREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\n        PREFIX vivo: &lt;http://vivoweb.org/ontology/core#&gt;\n        PREFIX vivo-de: &lt;http://vivoweb.org/ontology/core/de#&gt;\n        PREFIX vivo-bua: &lt;http://vivo.berlin-university-alliance.de/ontology/core/v1/vivo/bua#&gt;\n        PREFIX bua: &lt;http://vivo.berlin-university-alliance.de/ontology/core/v1/bua/&gt;\n        PREFIX obo: &lt;http://purl.obolibrary.org/obo/&gt;\n\n        SELECT DISTINCT ?uri ?label \n        WHERE \n        {\n           bua:bua obo:BFO_0000051 ?uri .\n           OPTIONAL { ?uri rdfs:label ?label . \n           FILTER (lang(?label) = \"de-de\") } .\n         }\n         ORDER BY ?label\n         \"\"\" .\n</code></pre> <ul> <li>Lines 2-10 are the namespaces needed to resolve the SPARQL query</li> <li>line 12 asks to select the uri identifier and the label of each entity</li> <li>line 15, asks to get the uri for any entity being playing the role of obo:BFO_0000051 (has part) for the bua entity</li> <li>line 16 -17 fetches the labels (names) for the entities in the german language perspective (de-de)</li> </ul> <p>The display:buaProjectDataGetter (fetch projects) getter function:</p> <pre><code>display:buaProjectDataGetter\n      a &lt;java:edu.cornell.mannlib.vitro.webapp.utils.dataGetter.SparqlQueryDataGetter&gt; ;\n      display:saveToVar \"buaProjekte\" ;\n      display:query \"\"\"\n      PREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\n      PREFIX vivo: &lt;http://vivoweb.org/ontology/core#&gt;\n\n      SELECT DISTINCT ?uri ?label\n      WHERE\n        {\n           ?uri a vivo:Project .\n           OPTIONAL { ?uri rdfs:label ?label} . \n         }\n         ORDER BY ?label\n      \"\"\" .\n</code></pre> <ul> <li>Lines 2-6 are the namespaces needed to resolve the SPARQL query</li> <li>line 8 asks to select the uri identifier and the label of each entity</li> <li>line 11, asks to get the uri for any entity instantiating the vivo:Project class</li> <li>line 12 fetches the labels (names) for the projects</li> </ul> <p>The display:exzellenzClustersDataGetter (fetch excellence clusters) getter function:</p> <pre><code>display:exzellenzClustersDataGetter\n    a &lt;java:edu.cornell.mannlib.vitro.webapp.utils.dataGetter.SparqlQueryDataGetter&gt; ;\n    display:saveToVar \"exzellenzClusters\" ;\n    display:query \"\"\"\n    PREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\n    PREFIX vivo: &lt;http://vivoweb.org/ontology/core#&gt;\n    PREFIX vivo-de: &lt;http://vivoweb.org/ontology/core/de#&gt;\n    PREFIX bua: &lt;http://vivo.berlin-university-alliance.de/ontology/core/v1/bua/&gt;\n\n    SELECT DISTINCT ?uri ?label \n    WHERE \n    {\n       ?uri a bua:Exzellenzcluster .\n       OPTIONAL { ?uri rdfs:label ?label . \n       FILTER (lang(?label) = \"en-us\") } .\n     }\n     ORDER BY ?label\n     \"\"\" .\n</code></pre> <ul> <li>Lines 2-8 are the namespaces needed to resolve the SPARQL query</li> <li>line 10 asks to select the uri identifier and the label of each entity</li> <li>line 13, asks to get the uri for any entity instantiating the  bua:Exzellenzcluster class</li> <li>line 14 -15 fetches the labels (names) for the entities in the english language perspective (de-de)</li> </ul>"},{"location":"vivo/#calls-to-the-functions-in-the-web-app-template-code","title":"Calls to the functions in the web app template code","text":""},{"location":"vivo/#templatesfreemarkerliblib-home-pageftl","title":"templates/freemarker/lib/lib-home-page.ftl:","text":"<p>Renders the html for the faculty member section on the home page, Works in conjunction with the homePageUtils.js file, which contains the ajax call The data is parsed from the variables created in the SPARQL queries of the homePageDataGetters.n3 file and made ready for calls from themes/tenderfoot/templates/page/page-home.ftl</p>"},{"location":"vivo/#jshomepageutilsjs","title":"js/homePageUtils.js","text":"<p>contains the functions</p> <ul> <li>buildExzellenzClusters()</li> <li>buildBuaMitglieder()</li> <li>buildBuaProjects()</li> </ul> <p>which build the html for rendering, reading the corresponding variables in the lib-home-page.ftl</p>"},{"location":"vivo/#themestenderfoottemplatespagepage-homeftl","title":"themes/tenderfoot/templates/page/page-home.ftl","text":"<p>Calls and renders the data generated in lib-home-page.ftl</p> <pre><code>&lt;div class=\"container\"&gt;\n    &lt;div class=\"col-md-4\"&gt;\n        &lt;!-- List List of bua members --&gt;\n        &lt;@lh.buaMitGliederHtml /&gt;\n    &lt;/div&gt;\n    &lt;div class=\"col-md-4\"&gt;\n        &lt;!-- List of exzellenzclusters --&gt;\n        &lt;@lh.exzellenzClusterHtml /&gt;\n    &lt;/div&gt;\n    &lt;div class=\"col-md-4\"&gt;\n        &lt;!-- List of projects --&gt;\n        &lt;@lh.buaProjectHtml /&gt;\n    &lt;/div&gt;\n&lt;/div&gt;\n</code></pre>"},{"location":"vivo/#language-files","title":"Language files","text":"<p>The templates read the strings for the labels in the web application from i18n language configuration files found in the  i18n folder, specifically, the </p> <ul> <li>vivo_all_de_DE.properties</li> <li>all_de_DE.properties</li> </ul> <p>and </p> <ul> <li>themes/tenderfoot/i18n/all_en_US.properties</li> </ul>"},{"location":"vivo/#css-alterations","title":"CSS ALTERATIONS","text":"<p>For the BUA VIVO project, some alterations and additions to some of the tenderfoot theme stylesheets where implemented:</p>"},{"location":"vivo/#themestenderfootcsspage-homecss","title":"themes/tenderfoot/css/page-home.css","text":"<pre><code>\n.jumbotron {\n    background-color: transparent;\n    color: #ffffff;\n    text-align: center;\n    text-shadow: 1px 1px 4px #153f6c;\n\n}\n\n.jumbotron p {\n    color: #fff;\n    text-shadow: 1px 1px 15px #153f6c, 1px 1px 5px #153f6c, 1px     1px 5px #153f6c;\n    margin-top: 0px;\n    bottom-top: 10px;\n}\n\n\n.jumbotron h1 {\n    font-size: 32px;\n    margin-top: 20px;\n} /*added by vivo-bua to change intro font size*/\n\n.hero {\n    /*background: #283a4b url(../images/hero-backgroundBUA.png) repeat-x 0 0;*/\n    background: linear-gradient(rgba(7,65,47, 0), rgba(7,65,47, 0)), url(../images/hero-backgroundBUA.png);\n  height: 100%;\n\n  /* Position and center the image to scale nicely on all screens */\n  background-position: center;\n  background-repeat: no-repeat;\n  background-size: cover;\n  position: relative;\n\n}\n\n.home-sections {\n    border-top: 1px dotted #dbe3e3; /* stroke */\n    border-bottom: 0px dotted #dbe3e3; /* stroke */\n    background-color: #fff; /* layer fill content */\n}\n\n.home-sections h4 {\n    border-top: 1px solid rgba(220,228,227,.42); /* stroke */\n    border-bottom: 1px solid rgba(220,228,227,.42); /* stroke */\n    /* background-color: #395d7f; /* layer fill content */\n    background-color: #4DB178;\n    border-bottom: 2px solid #86C8A3;\n    color: #fff;\n}\n\ndiv.faculty-home {\n    padding-top: 60px;\n}\n\nsection#home-bua-mitglieder &gt; div#bua-mitglieder &gt; ul {\n  margin: 0;\n  border: 0;\n  padding-left: 40px;\n  padding-top: 10px;\n}\n\nsection#home-exzellenz-cluster &gt; div#exzellenz-Clusters &gt; ul {\n  margin: 0;\n  border: 0;\n  padding-left: 40px;\n  padding-top: 10px;\n}\n\nsection#home-bua-project &gt; div#bua-projects &gt; ul {\n  margin: 0;\n  border: 0;\n  padding-left: 40px;\n  padding-top: 10px;\n}\n\nsection#home-bua-project &gt; div#bua-projects &gt; ul &gt; li {\n  padding-bottom: 5px;\n}\n\nsection#home-research &gt; ul {\n  margin: 0;\n  border: 0;\n  padding-left: 30px;\n  padding-top: 10px;\n}\n\n\n</code></pre>"},{"location":"vivo/#themestenderfootcsstenderfootcss","title":"themes/tenderfoot/css/tenderfoot.css","text":"<pre><code>\n.navbar-toggle .button-label {\n    display: inline-block;\n    float: left;\n    font-weight: bold;\n    color: #4EB178; \n    line-height: 14px;\n    padding-right: 10px;\n}\n\nbody {\n    padding: 0;\n    height: 100%; /* needed for container min-height */\n    font-family: \"Noto Sans\", \"Lucida Sans Unicode\",\"Lucida Grande\", Geneva, helvetica, sans-serif;\n    height: auto !important; /* real browsers */\n    height: 100%; /* IE6: treaded as min-height*/;\n    min-height: 100%; /* real browsers */\n    margin: 0 auto;\n   /* background: #f3f3f0 url(../images/header-backgroundBUA.png) center 0 no-repeat;*/\n\n  /*  background: linear-gradient(rgba(7,65,47, 1),rgba(7,65,47, 1), rgba(199,205,205,1)), url(../images/header-backgroundBUA.png);*/\n\n   /*  background: linear-gradient(180deg, #07412F, #07412F, #C7CDCD, #C7CDCD, #C7CDCD); */\n\n/* background: linear-gradient(180deg, #360860,#360860,#E2DEE6, #E2DEE6,#E2DEE6);*/\n\nbackground: linear-gradient(rgba(7,65,47, 1),rgba(7,65,47, 1), rgba(7,65,47, 0.5), rgba(211,215,215,1),rgba(211,215,215,1));\n\n  /* Position and center the image to scale nicely on all screens */\n  background-position: center;\n  background-repeat: no-repeat;\n  background-size: cover;\n  position: relative;\n\n}\n\n\n#wrapper-content {\n    background: repeat scroll 0 0 #fff;\n    padding-top: 20px;\n    padding-bottom: 20px;\n}\n\n.row.title {\n    background: #07412F;\n    padding-top: 7px;\n    padding-bottom: 3px;\n}\n\n.person-details {\n    background: #f3f3f;\n    padding-bottom: 5px;\n}\n\n\n#search-field {\n    width: 396px;\n    height: 38px;\n    background: url(../images/search-interior-pages.png) 0 0    no-repeat;\n    background-size: cover;\n}\n\n\n.icon-search {\n    text-decoration: none;\n    background-color: transparent;\n    color: #4DB178;\n    font-size: 14px;\n    border: none;\n    cursor: pointer;\n}\n\nfooter .row {\n    background-color: #07412F;\n    /*  background-color: black;\n    background-color: rgba(0, 0, 0, 0);*/\n}\n\n\nfooter p.partners {\n    text-align: center;\n    margin-top: 20px;\n    color: white;\n    font-size: 14px;\n}\n\nul#footer-nav {\n    float: right;\n    list-style: none;\n    height: 20px;\n    margin: 0;\n    padding: 0;\n    padding-top: 15px;\n}\n\n#footer-nav a:hover,\na.terms, a.partners,\na.powered-by-vivo {\n    color: white;\n    text-decoration: none;\n}\n\n\n#footer-nav  a.terms:hover, a.partners,\na.powered-by-vivo:hover {\n    color: white;\n    text-decoration: none;\n}\n\n/* BRANDING ------&gt;  BUA LOGO */\nh1.vivo-logo {\n    position: absolute;\n    width: 100%;\n    max-width: 442px;\n    height: 59px;\n    top: 30px;\n    left: 0;\n    /* background: url(../images/VIVO-logo.png) 0 0 no-repeat; */\n    background: url(../images/BUA_logo.svg) 0 0 no-repeat;\n    background-size: 100% auto;\n}\n\n\n</code></pre>"},{"location":"vivo/#bua-images","title":"BUA IMAGES","text":"<p>Some images specific to the BUA VIVO were created:</p> <ul> <li>themes/tenderfoot/images/BUA_logo.png</li> <li>themes/tenderfoot/images/BUA_logo.svg</li> <li>themes/tenderfoot/images/Footer.png</li> <li>themes/tenderfoot/images/header-backgroundBUA.png</li> <li>themes/tenderfoot/images/hero-backgroundBUA.png </li> </ul>"},{"location":"vivo/#after-the-installation-vivo-home","title":"After the Installation - vivo-home","text":"<p>Before using vivo-home for the first time, the runtime.properties must be adapted.</p> <pre><code>Vitro.defaultNamespace = http://vivo.berlin-university-alliance.de/individual/\n\nrootUser.emailAddress = xxx@yyy.zzz\n\nvitro.local.solr.url = http://192.168.10.20:8983/solr/vivocore\n</code></pre> <p>The rootUser.emailAddress is only relevant for the first login and may differ.</p> <p>The IP in the SolR URL must be adjusted. </p> <p>In addition, the location of the home folder has to be configured in the webapp in frontend-repository</p> <p>Open vivo-frontend/META-INF/context.xml and change the path of the Environment/value attribute:</p> <pre><code>&lt;Context&gt;\n   &lt;Environment\n           type=\"java.lang.String\"\n           name=\"vitro/home\"\n           value=\"....PATH TO ...../vivo-home-config\" override=\"true\"/&gt;\n\n   &lt;Manager pathname=\"\" /&gt;\n&lt;/Context&gt;\n</code></pre>"},{"location":"vivoserver/","title":"VIVO-Server","text":""},{"location":"vokabularen/","title":"Vocabularies","text":"<p>The vocabularies used in the Berlin University Alliance (BUA) VIVO project, are used not only to classify publications and other research outputs, but also carry the role of creating a more ample semantic context for the machine readability of the ontologies and the knowledge graphs. The vocabularies and the alliance wide research topic vocabulary, and the assignment of these to outputs as well as projects and institutional structures, form a network bridging semantically diverging entities.</p> <p>Querying and traversing the knowledge graph, it is possible to make inferences to the similarity of otherwise divergent entities assuming that their semantic likeness can be inferred across them being classified under the same themes and vocabulary topics.</p> <p> Fig. 1: Classification of Ontology entities; over bridging Vocabularies.</p> <p>Fig. 1 is a diagram showing the subsumption hierarchy of the institutional structures, their classification, as well as the relation between vocabularies and projects and research outputs in the BUA knowledge graph The vocabularies add an additional layer of semantic space for the ontologies and its instances in order to make automatic classification possible. Topics were then extracted from the different vocabularies, defining a semantic space for the vocabulary terms which in themselves are far too generic to be of any use for automatic classification.</p>"},{"location":"vokabularen/#fachersystematiken-des-statistischen-bundesamtes-destatis","title":"F\u00e4chersystematiken des Statistischen Bundesamtes (DESTATIS)","text":"<p>The Subject classifications of the German Federal Statistical Office, is a vocabulary of subject matter groups, intended for the classification of teaching and research entities. It is in our project intended for the classification of institutional entities in our Organization ontologies. In addition, the vocabulary is also the chosen vocabulary for the classification of institutional units, in accordance with the allocation of the organizational units done in the GERiT DFG portal. We have in the project, created a fork of the VIVO-de SKOS conversion, and translated it to English.</p>"},{"location":"vokabularen/#eudat-b2find-disciplinary-research-vocabulary","title":"eudat-b2find Disciplinary Research Vocabulary","text":"<p>The B2FIND is an interdisciplinary  discovery portal for research output. It is a comprehensive joint metadata catalogue and a powerful discovery portal. Metadata stored through EUDAT services such as B2SHARE are harvested from various research communities overarching a wide scope of research disciplines. Among these datasets, we have chosen the b2Find research disciplines vocabulary as a basis for our disciplinary classification of research outputs.</p> <p>As part of our project output, we converted the vocabulary into the SKOS format, and translated the terms into German.</p>"},{"location":"vokabularen/#interdisciplinary-research-field-classification","title":"Interdisciplinary research field classification","text":"<p>Classification for interdisciplinary research fields is a  Research field classification to support a differentiated representation of interdisciplinary or subject- or problem-related research on the basis of existing research field lists. The vocabulary was developed in collaboration project between the Humboldt University in Berlin and the German Center for University and Science Research.</p> <p>As we in the project dealt with research groups having a quite interdisciplinary scope, it was also clear that we would need to use this vocabulary in order to open up for a broader and discipline-overarching classification of research outputs and projects.</p> <p>In the project, we forked the kdsf-ffk project, and created an Englisch translation of it, which  adopted and proof read by the KFID and published  The vocabulary can be viewed at the SKOSHUB portal</p>"},{"location":"vorklassifikationspipelines/","title":"Vorklassifikation","text":""}]}